{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linzhi0918/deliberative-politics/blob/main/notebooks/Revised_03_Generating_labels_on_unlabeled_data_with_trained_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsdLG3xw-LgT"
      },
      "source": [
        "# Use saved pickled classifier to generate labels\n",
        "\n",
        "***REQUIRE PYTHON 2***\n",
        "***DO NOT USE PYTHON 3***\n",
        "\n",
        "This classifier is heavily (99%+) adapted from: Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM. Please cite the paper when using the following code.\n",
        "\n",
        "This code is used to:\n",
        "\n",
        "- Load the pre-trained classifier and associated files\n",
        "- Transform new input data into the correct format for the classifier.\n",
        "- Run the classifier on the transformed data and return results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. “Automated Hate Speech Detection and the Problem of Offensive Language.” ICWSM.\n",
        "\n",
        "https://github.com/t-davidson/hate-speech-and-offensive-language\n",
        "\n",
        "https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/classifier.py\n",
        "\n",
        "https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/final_classifier.ipynb"
      ],
      "metadata": {
        "id": "fxfUN7TRlz8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This codebook was run in Google Colab"
      ],
      "metadata": {
        "id": "dFfFx_EiD7zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hT1ErLj--jCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269447ba-420b-4f97-c822-ae3d00aac269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.25.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.13.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.1.0)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.8 virtualenv-20.25.0\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python2.7'\n"
          ]
        }
      ],
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv -p /usr/bin/python2.7 python2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uZpUVJ58-jsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f0fa00-de54-4a56-e505-563104cfc061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: python2/bin/activate: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!source python2/bin/activate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NdYqjQGiUOju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29133467-9d1a-4d44-e3f3-066e082f89ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpfhSHt2-LgW"
      },
      "source": [
        "## Loading packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gpOBiZKF-LgW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1613769-dcef-4d77-ec86-37c1ce09ecca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting convokit\n",
            "  Downloading convokit-3.0.0.tar.gz (183 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/183.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.3)\n",
            "Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
            "Collecting dill>=0.2.9 (from convokit)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n",
            "Collecting clean-text>=0.6.0 (from convokit)\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1 (from convokit)\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.1)\n",
            "Collecting pymongo>=4.0 (from convokit)\n",
            "  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n",
            "Collecting dnspython>=1.16.0 (from convokit)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.6.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.3)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=6044b38a907e4358718022fbcc717015ec98149932902bb2d21bb28099d19ba0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=582cc3c3da53ad93d9ce53dec78551c699c0aa3c82fbce53e4354d8ec1fe28ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n",
            "Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.7 dnspython-2.4.2 emoji-1.7.0 ftfy-6.1.3 msgpack-numpy-0.4.8 pymongo-4.6.1 unidecode-1.3.7\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@author: Kokil\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#!pip install spacy\n",
        "#!python3 -m spacy download en_core_web_lg\n",
        "#!python3 -m spacy download en_core_web_sm\n",
        "\n",
        "!pip install convokit\n",
        "from convokit import Corpus, Speaker, Utterance\n",
        "from convokit import download\n",
        "from convokit import TextParser\n",
        "from convokit import PolitenessStrategies\n",
        "import spacy\n",
        "import nltk\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "import sklearn.externals\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vI3XrcRl-LgY"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def is_number(tok):\n",
        "    try:\n",
        "        float(tok)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def spacy_tokenizer(text):\n",
        "    return [tok.text if not is_number(tok.text) else '_NUM_' for tok in nlp(text)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UJUhuEF7-LgY"
      },
      "outputs": [],
      "source": [
        "def extract_harbingers(df, X_col):\n",
        "\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/2015_Diplomacy_lexicon.json') as f:\n",
        "        features = json.loads(f.readline())\n",
        "\n",
        "    for feature in features:\n",
        "        harbingers = [harbinger.encode('ascii', 'ignore').decode('ascii').lower() for harbinger in features[feature]]\n",
        "        features[feature] = harbingers\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = str(text)\n",
        "        text = text.replace('\\'', '')\n",
        "        text = text.lower()\n",
        "        text = text.replace('{html}',\"\")\n",
        "        text = re.sub(re.compile('<.*?>'), '', text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub('[0-9]+', '', text)\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        text = \" \".join(tokens)\n",
        "        return text\n",
        "\n",
        "    def get_feature_frequency(text, feature):\n",
        "        count = 0\n",
        "        for harbinger in features[feature]:\n",
        "            count += text.count(harbinger)\n",
        "        return count\n",
        "\n",
        "    df['clean_text'] = df.apply(lambda row: clean_text(row[X_col]), axis=1)\n",
        "    for feature in features:\n",
        "        df[feature] = df.apply(lambda row: get_feature_frequency(row['clean_text'], feature), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XUXtZZGA-LgZ"
      },
      "outputs": [],
      "source": [
        "def preprocess(text_string):\n",
        "\n",
        "        space_pattern = '\\s+'\n",
        "        giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "        mention_regex = '@[\\w\\-]+'\n",
        "        parsed_text = re.sub(space_pattern, ' ', text_string)\n",
        "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
        "        parsed_text = re.sub(mention_regex, '', parsed_text)\n",
        "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
        "        return parsed_text\n",
        "\n",
        "\n",
        "#nlp = en_core_web_sm.load()\n",
        "#spacy.load(\"en_core_web_lg\")\n",
        "#spacy.load(\"en_core_web_sm\")\n",
        "ps = PolitenessStrategies()\n",
        "spacy_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "cols = list(ps.transform_utterance(\"hello, could you please help me proofread this article?\", spacy_nlp=spacy_nlp).meta['politeness_strategies'])\n",
        "\n",
        "def extract_politeness_feats(df, X_col):\n",
        "\n",
        "    def extract_politeness_helper(row):\n",
        "        utt = ps.transform_utterance(row[X_col], spacy_nlp=spacy_nlp)\n",
        "        feats = [utt.meta['politeness_strategies'][x] for x in cols]\n",
        "        return pd.Series(feats)\n",
        "\n",
        "    df[cols] = df.apply(extract_politeness_helper, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jZe3SMMJ-LgZ"
      },
      "outputs": [],
      "source": [
        "# List harbingers, liwc and politeness features\n",
        "import json\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/2015_Diplomacy_lexicon.json') as f:\n",
        "    harb_dict = json.loads(f.readline())\n",
        "politeness_dict = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/politeness_list.csv')\n",
        "liwc_dict = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/liwc_list.csv')\n",
        "X_cols = list(politeness_dict.columns) + list(liwc_dict.columns) + list(harb_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tbb_cy1C-LgZ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "happierfuntokenizer_v3\n",
        "\n",
        "This code implements a basic, Twitter-aware tokenizer.\n",
        "\n",
        "A tokenizer is a function that splits a string of text into words. In\n",
        "Python terms, we map string and unicode objects into lists of unicode\n",
        "objects.\n",
        "\n",
        "There is not a single right way to do tokenizing. The best method\n",
        "depends on the application.  This tokenizer is designed to be flexible\n",
        "and this easy to adapt to new domains and tasks.  The basic logic is\n",
        "this:\n",
        "\n",
        "1. The tuple regex_strings defines a list of regular expression\n",
        "   strings.\n",
        "\n",
        "2. The regex_strings strings are put, in order, into a compiled\n",
        "   regular expression object called word_re.\n",
        "\n",
        "3. The tokenization is done by word_re.findall(s), where s is the\n",
        "   user-supplied string, inside the tokenize() method of the class\n",
        "   Tokenizer.\n",
        "\n",
        "4. When instantiating Tokenizer objects, there is a single option:\n",
        "   preserve_case.  By default, it is set to True. If it is set to\n",
        "   False, then the tokenizer will downcase everything except for\n",
        "   emoticons.\n",
        "\n",
        "The __main__ method illustrates by tokenizing a few examples.\n",
        "\n",
        "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
        "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
        "and Twitter is cooperating, then it should tokenize a random\n",
        "English-language tweet.\n",
        "\"\"\"\n",
        "\n",
        "######################################################################\n",
        "\n",
        "import re\n",
        "import html.entities\n",
        "######################################################################\n",
        "# The following strings are components in the regular expression\n",
        "# that is used for tokenizing. It's important that phone_number\n",
        "# appears first in the final regex (since it can contain whitespace).\n",
        "# It also could matter that tags comes after emoticons, due to the\n",
        "# possibility of having text like\n",
        "#\n",
        "#     <:| and some text >:)\n",
        "#\n",
        "# Most imporatantly, the final element should always be last, since it\n",
        "# does a last ditch whitespace-based tokenization of whatever is left.\n",
        "\n",
        "# This particular element is used in a couple ways, so we define it\n",
        "# with a name:\n",
        "emoticon_string = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8>]                    # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8<]                    # eyes\n",
        "      [<>]?\n",
        "      |\n",
        "      <[/\\\\]?3                         # heart(added: has)\n",
        "      |\n",
        "      \\(?\\(?\\#?                   #left cheeck\n",
        "      [>\\-\\^\\*\\+o\\~]              #left eye\n",
        "      [\\_\\.\\|oO\\,]                #nose\n",
        "      [<\\-\\^\\*\\+o\\~]              #right eye\n",
        "      [\\#\\;]?\\)?\\)?               #right cheek\n",
        "    )\"\"\"\n",
        "\n",
        "# The components of the tokenizer:\n",
        "regex_strings = (\n",
        "    # Phone numbers:\n",
        "    r\"\"\"\n",
        "    (?:\n",
        "      (?:            # (international)\n",
        "        \\+?[01]\n",
        "        [\\-\\s.]*\n",
        "      )?\n",
        "      (?:            # (area code)\n",
        "        [\\(]?\n",
        "        \\d{3}\n",
        "        [\\-\\s.\\)]*\n",
        "      )?\n",
        "      \\d{3}          # exchange\n",
        "      [\\-\\s.]*\n",
        "      \\d{4}          # base\n",
        "    )\"\"\"\n",
        "    ,\n",
        "    # Emoticons:\n",
        "    emoticon_string\n",
        "    ,\n",
        "    # http:\n",
        "    # Web Address:\n",
        "    r\"\"\"(?:(?:http[s]?\\:\\/\\/)?(?:[\\w\\_\\-]+\\.)+(?:com|net|gov|edu|info|org|ly|be|gl|co|gs|pr|me|cc|us|gd|nl|ws|am|im|fm|kr|to|jp|sg)(?:\\/[\\s\\b$])?)\"\"\"\n",
        "    ,\n",
        "    r\"\"\"(?:http[s]?\\:\\/\\/)\"\"\"   #need to capture it alone sometimes\n",
        "    ,\n",
        "    #command in parens:\n",
        "    r\"\"\"(?:\\[[\\w_]+\\])\"\"\"   #need to capture it alone sometimes\n",
        "    ,\n",
        "    # HTTP GET Info\n",
        "    r\"\"\"(?:\\/\\w+\\?(?:\\;?\\w+\\=\\w+)+)\"\"\"\n",
        "    ,\n",
        "    # HTML tags:\n",
        "    r\"\"\"(?:<[^>]+\\w=[^>]+>|<[^>]+\\s\\/>|<[^>\\s]+>?|<?[^<\\s]+>)\"\"\"\n",
        "    #r\"\"\"(?:<[^>]+\\w+[^>]+>|<[^>\\s]+>?|<?[^<\\s]+>)\"\"\"\n",
        "    ,\n",
        "    # Twitter username:\n",
        "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
        "    ,\n",
        "    # Twitter hashtags:\n",
        "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        "    ,\n",
        "    # Remaining word types:\n",
        "    r\"\"\"\n",
        "    (?:[\\w][\\w'\\-_]+[\\w])       # Words with apostrophes or dashes.\n",
        "    |\n",
        "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
        "    |\n",
        "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
        "    |\n",
        "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
        "    |\n",
        "    (?:\\S)                         # Everything else that isn't whitespace.\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "######################################################################\n",
        "# This is the core tokenizing regex:\n",
        "\n",
        "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
        "\n",
        "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
        "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
        "\n",
        "# These are for regularizing HTML entities to Unicode:\n",
        "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
        "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
        "amp = \"&amp;\"\n",
        "\n",
        "hex_re = re.compile(r'\\\\x[0-9a-z]{1,4}')\n",
        "\n",
        "######################################################################\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, preserve_case=False, use_unicode=True):\n",
        "        self.preserve_case = preserve_case\n",
        "        self.use_unicode = use_unicode\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        \"\"\"\n",
        "        Argument: s -- any string or unicode object\n",
        "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
        "        \"\"\"\n",
        "        # Try to ensure unicode:\n",
        "        if self.use_unicode:\n",
        "            try:\n",
        "                s = str(s)\n",
        "            except UnicodeDecodeError:\n",
        "                s = str(s).encode('string_escape')\n",
        "                s = str(s)\n",
        "        # Fix HTML character entitites:\n",
        "        s = self.__html2unicode(s)\n",
        "        s = self.__removeHex(s)\n",
        "        # Tokenize:\n",
        "        words = word_re.findall(s)\n",
        "        #print words #debug\n",
        "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
        "        if not self.preserve_case:\n",
        "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
        "\n",
        "        return words\n",
        "\n",
        "\n",
        "    def __html2unicode(self, s):\n",
        "        \"\"\"\n",
        "        Internal metod that seeks to replace all the HTML entities in\n",
        "        s with their corresponding unicode characters.\n",
        "        \"\"\"\n",
        "        # First the digits:\n",
        "        ents = set(html_entity_digit_re.findall(s))\n",
        "        if len(ents) > 0:\n",
        "            for ent in ents:\n",
        "                entnum = ent[2:-1]\n",
        "                try:\n",
        "                    entnum = int(entnum)\n",
        "                    s = s.replace(ent, chr(entnum))\n",
        "                except:\n",
        "                    pass\n",
        "        # Now the alpha versions:\n",
        "        ents = set(html_entity_alpha_re.findall(s))\n",
        "        ents = filter((lambda x : x != amp), ents)\n",
        "        for ent in ents:\n",
        "            entname = ent[1:-1]\n",
        "            try:\n",
        "                s = s.replace(ent, chr(htmlentitydefs.name2codepoint[entname]))\n",
        "            except:\n",
        "                pass\n",
        "            s = s.replace(amp, \" and \")\n",
        "        return s\n",
        "\n",
        "    def __removeHex(self, s):\n",
        "        return hex_re.sub(' ', s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bkaoe_xg-Lga"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import requests\n",
        "from io import StringIO\n",
        "def LeXmo(text,document,dictionary):\n",
        "\n",
        "    '''\n",
        "      Takes text and adds if to a dictionary with 10 Keys  for each of the 10 emotions in the NRC Emotion Lexicon,\n",
        "      each dictionay contains the value of the text in that emotions divided to the text word count\n",
        "      INPUT: string\n",
        "      OUTPUT: dictionary with the text and the value of 10 emotions\n",
        "      '''\n",
        "    reponse = \"\"\n",
        "    choice = 0\n",
        "    df = pd.DataFrame()\n",
        "    emodic = {'text': text}\n",
        "    if(dictionary == \"all\"):\n",
        "        #first nrc\n",
        "        choice = 1\n",
        "    if(dictionary == \"nrc\" or choice == 1):\n",
        "        response = requests.get('https://raw.github.com/dinbav/LeXmo/master/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')\n",
        "        nrc = StringIO(response.text)\n",
        "\n",
        "\n",
        "#        emodic = {'anger': [], 'anticipation': [], 'disgust': [], 'fear': [], 'joy': [], 'negative': [],                  'positive': [], 'sadness': [], 'surprise': [], 'trust': []}\n",
        "        thisdic =  {'anger': [], 'anticipation': [], 'disgust': [], 'fear': [], 'joy': [], 'negative': [],'positive': [], 'sadness': [], 'surprise': [], 'trust': []}\n",
        "        emodic = Merge(emodic,thisdic)\n",
        "\n",
        "        lexicon = pd.read_csv(nrc,\n",
        "                            names=[\"word\", \"emotion\", \"association\"],\n",
        "                            sep=r'\\t', engine='python')\n",
        "        df = df.append(lexicon)\n",
        "\n",
        "    if(dictionary == \"liwc\" or choice == 1):\n",
        "        #response = requests.get('/home/kokil/feature_extraction/data/liwc2015.txt')\n",
        "        #liwc = StringIO(response.text)\n",
        "        liwc = '/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/liwc2015.txt'\n",
        "\n",
        "\n",
        "       # emodic = {'text': text, 'PPRON': [],'BODY': [],'WE': [],'DEATH': [],'FOCUSFUTURE': [],'FEEL': [],'INTERROG': [],'NUMBER': [],'POSEMO': [],'NEGATE': [],'QUANT': [],'THEY': [],'AFFECT': [],'RELATIV': [],'HOME': [],'CONJ': [],'COGPROC': [],'SEXUAL': [],'AUXVERB': [],'SHEHE': [],'BIO': [],'DIFFER': [],'POWER': [],'NETSPEAK': [],'INFORMAL': [],'CAUSE': [],'FILLER': [],'INSIGHT': [],'LEISURE': [],'NEGEMO': [],'MOTION': [],'SEE': [],'FOCUSPAST': [],'ANGER': [],'ARTICLE': [],'NONFLU': [],'MALE': [],'WORK': [],'FRIEND': [],'FUNCTION': [],'RISK': [],'FAMILY': [],'SPACE': [],'I': [],'IPRON': [],'SOCIAL': [],'ASSENT': [],'DRIVES': [],'PERCEPT': [],'VERB': [],'HEAR': [],'FEMALE': [],'DISCREP': [],'YOU': [],'ADJ': [],'ACHIEVE': [],'RELIG': [],'TENTAT': [],'COMPARE': [],'ADVERB': [],'PRONOUN': [],'MONEY': [],'FOCUSPRESENT': [],'INGEST': [],'AFFILIATION': [],'SWEAR': [],'HEALTH': [],'SAD': [],'TIME': [],'REWARD': [],'ANX': [],'PREP': [],'CERTAIN': []}\n",
        "        thisdic = {'PPRON': [],'BODY': [],'WE': [],'DEATH': [],'FOCUSFUTURE': [],'FEEL': [],'INTERROG': [],'NUMBER': [],'POSEMO': [],'NEGATE': [],'QUANT': [],'THEY': [],'AFFECT': [],'RELATIV': [],'HOME': [],'CONJ': [],'COGPROC': [],'SEXUAL': [],'AUXVERB': [],'SHEHE': [],'BIO': [],'DIFFER': [],'POWER': [],'NETSPEAK': [],'INFORMAL': [],'CAUSE': [],'FILLER': [],'INSIGHT': [],'LEISURE': [],'NEGEMO': [],'MOTION': [],'SEE': [],'FOCUSPAST': [],'ANGER': [],'ARTICLE': [],'NONFLU': [],'MALE': [],'WORK': [],'FRIEND': [],'FUNCTION': [],'RISK': [],'FAMILY': [],'SPACE': [],'I': [],'IPRON': [],'SOCIAL': [],'ASSENT': [],'DRIVES': [],'PERCEPT': [],'VERB': [],'HEAR': [],'FEMALE': [],'DISCREP': [],'YOU': [],'ADJ': [],'ACHIEVE': [],'RELIG': [],'TENTAT': [],'COMPARE': [],'ADVERB': [],'PRONOUN': [],'MONEY': [],'FOCUSPRESENT': [],'INGEST': [],'AFFILIATION': [],'SWEAR': [],'HEALTH': [],'SAD': [],'TIME': [],'REWARD': [],'ANX': [],'PREP': [],'CERTAIN': []}\n",
        "        emodic = Merge(emodic,thisdic)\n",
        "        lexicon = pd.read_csv(liwc,\n",
        "                            names=[\"word\", \"emotion\", \"association\"],\n",
        "                            sep=r'\\t', engine='python')\n",
        "        df = df.append(lexicon)\n",
        "\n",
        "    if(dictionary == \"delib\" or choice == 1):\n",
        "        delib = '/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/dd_delib.txt'\n",
        "        #response = requests.get('/home/kokil/feature_extraction/data/dd_delib.txt')\n",
        "        #delib = StringIO(response.text)\n",
        "\n",
        "\n",
        "\n",
        "       # emodic = {'text': text, 'EMP_RES': [],'UNCIVIL_ABUSE': [],'CONSTRUCTIVENESS': [],'JUSTIFICATION': [],'RECIPROCITY': [],'JUST_EXT': [],'RELEVANCE': [],'JUST_INT': []}\n",
        "        thisdic =  {'EMP_RES': [],'UNCIVIL_ABUSE': [],'CONSTRUCTIVENESS': [],'JUSTIFICATION': [],'RECIPROCITY': [],'JUST_EXT': [],'RELEVANCE': [],'JUST_INT': []}\n",
        "        emodic = Merge(emodic,thisdic)\n",
        "\n",
        "\n",
        "        lexicon = pd.read_csv(delib,\n",
        "                            names=[\"word\", \"emotion\", \"association\"],\n",
        "                            sep=r'\\t', engine='python')\n",
        "        df = df.append(lexicon)\n",
        "    if(dictionary == \"hate\" or choice == 1):\n",
        "        hate = '/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/incivilities.txt'\n",
        "        #response = requests.get('/home/kokil/feature_extraction/data/incivilities.txt')\n",
        "        #hate = StringIO(response.text)\n",
        "\n",
        "\n",
        "\n",
        "        #emodic = {'text': text, 'SWEAR': [],'UNCIV': [],'OFFEN': []}\n",
        "        thisdic = {'UNCIV': [],'OFFEN': []}\n",
        "        emodic = Merge(emodic,thisdic)\n",
        "\n",
        "        lexicon = pd.read_csv(hate,           names=[\"word\", \"emotion\", \"association\"],             sep=r'\\t', engine='python')\n",
        "        df = df.append(lexicon)\n",
        "\n",
        "    df = df.drop_duplicates(subset=['word', 'emotion'])\n",
        "    df.reset_index()\n",
        "    emolex_words = df.pivot(index='word',\n",
        "                                   columns='emotion',\n",
        "                                   values='association').reset_index()\n",
        "    emolex_words.drop(emolex_words.index[0])\n",
        "\n",
        "    categories = emolex_words.columns.drop('word')\n",
        "\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "    rows_list = []\n",
        "    word_count = len(document)\n",
        "    for word in document:\n",
        "            word = stemmer.stem(word.lower())\n",
        "\n",
        "            emo_score = (emolex_words[emolex_words.word == word])\n",
        "            rows_list.append(emo_score)\n",
        "\n",
        "\n",
        "    df = pd.concat(rows_list)\n",
        "    df.reset_index(drop=True)\n",
        "\n",
        "    for category in list(categories):\n",
        "        emodic[category] = df[category].sum() / word_count\n",
        "\n",
        "    return emodic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ezgMKk9k-Lgb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def tokenize_messages(filename,col_text,col_msgid):\n",
        "    with open(filename,encoding=\"utf-8\") as corpus:\n",
        "            reader = csv.reader(corpus)\n",
        "    #splitsfile = open('C:/Users/User/Dropbox/Content Analysis/Corpus/Full Corpus/fullcorpus_split.csv','a',newline='',encoding=\"utf-8\")\n",
        "    #f_revs = csv.writer(splitsfile)\n",
        "    #f_revs.writerow([\"message_id\",\"SITE ID\",\"message\",\"Like Count\",\"postlength\"])\n",
        "\n",
        "            rows_list = []\n",
        "            for row in reader:\n",
        "                message = row[col_text]\n",
        "                tokenizer = Tokenizer(preserve_case=True)\n",
        "                words = tokenizer.tokenize(message.lower())\n",
        "                #print(words)\n",
        "                totalGrams=0\n",
        "                freqs = dict()\n",
        "                totalChars = 0\n",
        "                gram = ''\n",
        "                for n in range (1,4):\n",
        "                    for i in range(0,(len(words) - n)+1):\n",
        "                        totalGrams += 1\n",
        "                        gram = ' '.join(words[i:i+n])\n",
        "                        try:\n",
        "                            freqs[gram] = 1\n",
        "                        except:\n",
        "                            print(\"error\")\n",
        "                freqs[\"message_id\"]=row[col_msgid]\n",
        "                rows_list.append(freqs)\n",
        "            df = pd.DataFrame(rows_list)\n",
        "            df= df.replace(np.nan, 0)\n",
        "            print(\"Writing tokenized messages to csv...\")\n",
        "            timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
        "            #print timestr\n",
        "            df.to_csv(\"tokenized_messages_\"+timestr+\".csv\")\n",
        "            return df\n",
        "\n",
        "############################\n",
        "\n",
        "def emolize_messages(filename,col_text,col_msgid,choice):\n",
        "    with open(filename,encoding=\"utf-8\") as corpus:\n",
        "            reader = csv.reader(corpus)\n",
        "    #splitsfile = open('C:/Users/User/Dropbox/Content Analysis/Corpus/Full Corpus/fullcorpus_split.csv','a',newline='',encoding=\"utf-8\")\n",
        "    #f_revs = csv.writer(splitsfile)\n",
        "    #f_revs.writerow([\"message_id\",\"SITE ID\",\"message\",\"Like Count\",\"postlength\"])\n",
        "\n",
        "            rows_list = []\n",
        "            for row in reader:\n",
        "                message = row[col_text]\n",
        "                tokenizer = Tokenizer(preserve_case=True)\n",
        "                words = tokenizer.tokenize(message.lower())\n",
        "                emodic = LeXmo(message.lower(),words,choice)\n",
        "                print(emodic)\n",
        "                rows_list.append(emodic)\n",
        "                #print(emodic)\n",
        "            df = pd.DataFrame(rows_list)\n",
        "            df= df.replace(np.nan, 0)\n",
        "            print(\"Writing emolized messages to csv...\")\n",
        "            timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
        "            #print timestr\n",
        "            df.to_csv(filename+\"_\"+choice+\"_\"+timestr+\".csv\")\n",
        "            return df\n",
        "\n",
        "############################\n",
        "def extract_counts(df,X_col):\n",
        "            vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, stop_words=STOP_WORDS, strip_accents='unicode')\n",
        "            corpus = list(df[X_col].str.lower())\n",
        "            X = vectorizer.fit_transform(corpus)\n",
        "            df = df.join(pd.DataFrame(X.toarray()).add_prefix('count_'))\n",
        "            df.to_csv(os.path.join(MODIFIED_DATA, '/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/modified_data/counts.csv'))\n",
        "def extract_tfidf(df,X_col):\n",
        "            vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer, stop_words=STOP_WORDS, strip_accents='unicode',max_features = 10000)\n",
        "            corpus = list(df[X_col].str.lower())\n",
        "            X = vectorizer.fit_transform(corpus)\n",
        "            df = df.join(pd.DataFrame(X.toarray()).add_prefix('tfidf_'))\n",
        "            df.to_csv(os.path.join(MODIFIED_DATA, '/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/modified_data/tfidf.csv'))\n",
        "            return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tfidf_and_pos(df,X_col):\n",
        "            print(\"Loading other information...\")\n",
        "            tf_vectorizer = joblib.load('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/final_tfidf_vectorizer.pkl')\n",
        "            idf_vector = joblib.load('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/final_idf_vectorizer.pkl')\n",
        "            pos_vectorizer = joblib.load('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/final_pos_vectorizer.pkl')\n",
        "            tweets = df[X_col]\n",
        "\n",
        "            X = transform_inputs(df[X_col], tf_vectorizer, idf_vector, pos_vectorizer) # Error: Function \"transform_inputs\" is not defined.\n",
        "            featurenames = vectorizer.get_feature_names() + pos_vectorizer.get_feature_names() # Error: \"vectorizer\" is not defined\n",
        "            df_tfidf = pd.DataFrame(tfidfs.toarray(), columns= featurenames) # Error: \"tfidfs\" is not defined\n",
        "            df = df.join(df_tfidf)\n",
        "\n",
        "            return df\n",
        "\n",
        "        #print(tokens[i],tag_list[i])"
      ],
      "metadata": {
        "id": "0ghxZ87CQwQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tnmZQczG-Lgc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# code\n",
        "# Python code to merge dict using a single\n",
        "# expression\n",
        "def Merge(dict1, dict2):\n",
        "    res = {**dict1, **dict2}\n",
        "    return res\n",
        "\n",
        "def extract_feats(filename, X_col):\n",
        "    df = emolize_messages(filename,1,0,\"all\")\n",
        "    extract_harbingers(df, X_col)\n",
        "    extract_politeness_feats(df, X_col)\n",
        "    #df = extract_tfidf_and_pos(df,X_col)\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
        "    #print timestr\n",
        "    #df.to_csv(filename+\"_allfeats_\"+timestr+\".csv\")\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D4tWqoFE-Lgd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5a76ffc-c73c-45b0-a466-5b2a326bae83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '@mcfaul it invaded ukraine long time ago. today putin just made it official.', 'anger': 0.0, 'anticipation': 0.13333333333333333, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0, 'positive': 0.0, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.0, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.13333333333333333, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.06666666666666667, 'SEXUAL': 0.0, 'AUXVERB': 0.0, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.06666666666666667, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.13333333333333333, 'ANGER': 0.0, 'ARTICLE': 0.0, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.2, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.06666666666666667, 'I': 0.0, 'IPRON': 0.13333333333333333, 'SOCIAL': 0.0, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.0, 'VERB': 0.06666666666666667, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.06666666666666667, 'PRONOUN': 0.13333333333333333, 'MONEY': 0.0, 'FOCUSPRESENT': 0.0, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.06666666666666667, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.0, 'CERTAIN': 0.0, 'EMP_RES': 0.012375183266666666, 'UNCIVIL_ABUSE': -0.022317602800000003, 'CONSTRUCTIVENESS': 0.030696713133333334, 'JUSTIFICATION': 0.054689542066666666, 'RECIPROCITY': -0.015255888266666665, 'JUST_EXT': -0.0485659138, 'RELEVANCE': 0.06619200786666667, 'JUST_INT': -0.00019968413333333336, 'UNCIV': -0.022317600000000003, 'OFFEN': 0.172768652}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'an unusual thing just happened in @abcnews. they cut into a story with the #crimeminister speaking word salad about ukraine to go to a live presser with @albomp! and they didn’t cut it! i nearly fell off my chair. pretty sure the work experience kid is in charge today. #auspol', 'anger': 0.0, 'anticipation': 0.017543859649122806, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.017543859649122806, 'positive': 0.017543859649122806, 'sadness': 0.017543859649122806, 'surprise': 0.0, 'trust': 0.017543859649122806, 'PPRON': 0.07017543859649122, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.03508771929824561, 'AFFECT': 0.03508771929824561, 'RELATIV': 0.12280701754385964, 'HOME': 0.0, 'CONJ': 0.017543859649122806, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.017543859649122806, 'SHEHE': 0.0, 'BIO': 0.017543859649122806, 'DIFFER': 0.0, 'POWER': 0.017543859649122806, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.03508771929824561, 'MOTION': 0.03508771929824561, 'SEE': 0.0, 'FOCUSPAST': 0.017543859649122806, 'ANGER': 0.0, 'ARTICLE': 0.08771929824561403, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.017543859649122806, 'FRIEND': 0.0, 'FUNCTION': 0.40350877192982454, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.08771929824561403, 'I': 0.03508771929824561, 'IPRON': 0.017543859649122806, 'SOCIAL': 0.07017543859649122, 'ASSENT': 0.0, 'DRIVES': 0.03508771929824561, 'PERCEPT': 0.017543859649122806, 'VERB': 0.10526315789473684, 'HEAR': 0.017543859649122806, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.017543859649122806, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.03508771929824561, 'PRONOUN': 0.08771929824561403, 'MONEY': 0.0, 'FOCUSPRESENT': 0.10526315789473684, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.017543859649122806, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.17543859649122806, 'CERTAIN': 0.0, 'EMP_RES': 0.02350822784210526, 'UNCIVIL_ABUSE': -0.013123567526315789, 'CONSTRUCTIVENESS': 0.00473009947368421, 'JUSTIFICATION': 0.06136882905263158, 'RECIPROCITY': -0.008523541999999999, 'JUST_EXT': -0.05737821740350877, 'RELEVANCE': 0.0776882386140351, 'JUST_INT': 0.00936404838596491, 'UNCIV': -0.01312356456140351, 'OFFEN': 0.32285366491228074}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '@johnrich and your a fool!!! putin loves you cause your an idiot!!!🇺🇸🇺🇸🇺🇸', 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.08333333333333333, 'fear': 0.0, 'joy': 0.041666666666666664, 'negative': 0.08333333333333333, 'positive': 0.041666666666666664, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.125, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.041666666666666664, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.08333333333333333, 'RELATIV': 0.0, 'HOME': 0.0, 'CONJ': 0.041666666666666664, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.0, 'SHEHE': 0.0, 'BIO': 0.041666666666666664, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.041666666666666664, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.08333333333333333, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.25, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.0, 'I': 0.0, 'IPRON': 0.0, 'SOCIAL': 0.20833333333333334, 'ASSENT': 0.0, 'DRIVES': 0.041666666666666664, 'PERCEPT': 0.0, 'VERB': 0.0, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.125, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.0, 'PRONOUN': 0.125, 'MONEY': 0.0, 'FOCUSPRESENT': 0.0, 'INGEST': 0.0, 'AFFILIATION': 0.041666666666666664, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.0, 'CERTAIN': 0.0, 'EMP_RES': -0.005548112625, 'UNCIVIL_ABUSE': 0.015787509875, 'CONSTRUCTIVENESS': -0.017684948999999995, 'JUSTIFICATION': 0.008003549083333332, 'RECIPROCITY': -0.03926819554166666, 'JUST_EXT': -0.08609560725, 'RELEVANCE': 0.06947892708333332, 'JUST_INT': 0.012123718333333333, 'UNCIV': 0.0157875125, 'OFFEN': 0.3842554583333333}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '#china\\'s un statement: \"china has been paying close attention to the latest developments of the situation in ukraine we\\'ve fully elaborated on our position at previous 2 meetings. at present all parties must exercise restraint and avoid any action that may fuel tensions --', 'anger': 0.020833333333333332, 'anticipation': 0.0625, 'disgust': 0.0, 'fear': 0.020833333333333332, 'joy': 0.041666666666666664, 'negative': 0.020833333333333332, 'positive': 0.125, 'sadness': 0.0, 'surprise': 0.020833333333333332, 'trust': 0.0625, 'PPRON': 0.020833333333333332, 'BODY': 0.0, 'WE': 0.020833333333333332, 'DEATH': 0.0, 'FOCUSFUTURE': 0.020833333333333332, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.020833333333333332, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.14583333333333334, 'HOME': 0.0, 'CONJ': 0.020833333333333332, 'COGPROC': 0.0625, 'SEXUAL': 0.0, 'AUXVERB': 0.08333333333333333, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.020833333333333332, 'ANGER': 0.0, 'ARTICLE': 0.041666666666666664, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.3125, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.10416666666666667, 'I': 0.0, 'IPRON': 0.020833333333333332, 'SOCIAL': 0.041666666666666664, 'ASSENT': 0.0, 'DRIVES': 0.041666666666666664, 'PERCEPT': 0.0, 'VERB': 0.10416666666666667, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.020833333333333332, 'YOU': 0.0, 'ADJ': 0.020833333333333332, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.020833333333333332, 'COMPARE': 0.020833333333333332, 'ADVERB': 0.0, 'PRONOUN': 0.041666666666666664, 'MONEY': 0.0, 'FOCUSPRESENT': 0.0625, 'INGEST': 0.0, 'AFFILIATION': 0.041666666666666664, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.041666666666666664, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.125, 'CERTAIN': 0.041666666666666664, 'EMP_RES': 0.012704617395833333, 'UNCIVIL_ABUSE': -0.0162386923125, 'CONSTRUCTIVENESS': 0.010360593708333332, 'JUSTIFICATION': 0.04876083716666666, 'RECIPROCITY': 0.000815226916666667, 'JUST_EXT': -0.03275176145833334, 'RELEVANCE': 0.055613144208333326, 'JUST_INT': 0.0063005585, 'UNCIV': -0.016238697708333336, 'OFFEN': 0.1770797120833333}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'putin i am not kidding is blaming bill clinton for not letting russia into nato.', 'anger': 0.0625, 'anticipation': 0.0, 'disgust': 0.0625, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0625, 'positive': 0.0, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.0625, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.125, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.0625, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.125, 'SEXUAL': 0.0, 'AUXVERB': 0.1875, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.125, 'POWER': 0.0625, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.0, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.5, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.0625, 'I': 0.0625, 'IPRON': 0.0, 'SOCIAL': 0.0625, 'ASSENT': 0.0, 'DRIVES': 0.0625, 'PERCEPT': 0.0, 'VERB': 0.1875, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.0, 'PRONOUN': 0.0625, 'MONEY': 0.0625, 'FOCUSPRESENT': 0.125, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.125, 'CERTAIN': 0.0, 'EMP_RES': 0.0012030356874999995, 'UNCIVIL_ABUSE': -0.0401616471875, 'CONSTRUCTIVENESS': 0.006049147812499999, 'JUSTIFICATION': 0.07121243931250001, 'RECIPROCITY': -0.005566160562499999, 'JUST_EXT': -0.045264528125, 'RELEVANCE': 0.0712676265, 'JUST_INT': 0.013642780624999998, 'UNCIV': -0.040161643749999996, 'OFFEN': 0.33434518937499996}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '💯 % correct ', 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0, 'positive': 0.0, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.0, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.0, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.0, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.0, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.0, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.0, 'I': 0.0, 'IPRON': 0.0, 'SOCIAL': 0.0, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.0, 'VERB': 0.0, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.0, 'PRONOUN': 0.0, 'MONEY': 0.0, 'FOCUSPRESENT': 0.0, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.0, 'CERTAIN': 0.0, 'EMP_RES': 0.0, 'UNCIVIL_ABUSE': 0.0, 'CONSTRUCTIVENESS': 0.0, 'JUSTIFICATION': -0.0027608926666666668, 'RECIPROCITY': 0.006230523000000001, 'JUST_EXT': 0.002023638, 'RELEVANCE': 0.01044731, 'JUST_INT': 0.042157282, 'UNCIV': 0.0, 'OFFEN': 0.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'dear @justintrudeau:\\n\\nyou just imposed martial law in canada.\\n\\nyou may want to sit this one out. ', 'anger': 0.05, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0, 'positive': 0.05, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.05, 'PPRON': 0.1, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.05, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.05, 'POSEMO': 0.05, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.05, 'RELATIV': 0.15, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.1, 'SEXUAL': 0.0, 'AUXVERB': 0.05, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.0, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.05, 'FUNCTION': 0.4, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.15, 'I': 0.0, 'IPRON': 0.05, 'SOCIAL': 0.15, 'ASSENT': 0.0, 'DRIVES': 0.05, 'PERCEPT': 0.0, 'VERB': 0.15, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.05, 'YOU': 0.1, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.05, 'COMPARE': 0.0, 'ADVERB': 0.05, 'PRONOUN': 0.15, 'MONEY': 0.0, 'FOCUSPRESENT': 0.1, 'INGEST': 0.0, 'AFFILIATION': 0.05, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.15, 'CERTAIN': 0.0, 'EMP_RES': 0.025265642449999997, 'UNCIVIL_ABUSE': -0.0372106609, 'CONSTRUCTIVENESS': -0.006871970499999996, 'JUSTIFICATION': 0.0774163728, 'RECIPROCITY': 0.0022641422000000007, 'JUST_EXT': -0.076073839, 'RELEVANCE': 0.09780974814999999, 'JUST_INT': 0.0078295347, 'UNCIV': -0.037210639999999996, 'OFFEN': 0.39499176}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'i have signed an executive order to deny russia the chance to profit from its blatant violations of international law. we are continuing to closely consult with allies and partners including ukraine on next steps. ', 'anger': 0.02702702702702703, 'anticipation': 0.0, 'disgust': 0.02702702702702703, 'fear': 0.0, 'joy': 0.0, 'negative': 0.02702702702702703, 'positive': 0.02702702702702703, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.05405405405405406, 'PPRON': 0.05405405405405406, 'BODY': 0.0, 'WE': 0.02702702702702703, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.13513513513513514, 'HOME': 0.0, 'CONJ': 0.02702702702702703, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.05405405405405406, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.05405405405405406, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.02702702702702703, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.05405405405405406, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.40540540540540543, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.05405405405405406, 'I': 0.02702702702702703, 'IPRON': 0.02702702702702703, 'SOCIAL': 0.02702702702702703, 'ASSENT': 0.0, 'DRIVES': 0.08108108108108109, 'PERCEPT': 0.0, 'VERB': 0.05405405405405406, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.02702702702702703, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.0, 'PRONOUN': 0.08108108108108109, 'MONEY': 0.0, 'FOCUSPRESENT': 0.05405405405405406, 'INGEST': 0.0, 'AFFILIATION': 0.02702702702702703, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.02702702702702703, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.1891891891891892, 'CERTAIN': 0.0, 'EMP_RES': 0.018254719621621618, 'UNCIVIL_ABUSE': -0.024068120918918922, 'CONSTRUCTIVENESS': 0.0006817220000000003, 'JUSTIFICATION': 0.061145627, 'RECIPROCITY': -0.003541441486486486, 'JUST_EXT': -0.05703773681081081, 'RELEVANCE': 0.08222913494594594, 'JUST_INT': 0.00616670072972973, 'UNCIV': -0.02406812486486486, 'OFFEN': 0.23254668027027026}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'as if on cue @tuckercarlson shows his true colors. red white and blue…but of a different stripe. ', 'anger': 0.0, 'anticipation': 0.09523809523809523, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.09523809523809523, 'negative': 0.047619047619047616, 'positive': 0.09523809523809523, 'sadness': 0.047619047619047616, 'surprise': 0.0, 'trust': 0.14285714285714285, 'PPRON': 0.047619047619047616, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.047619047619047616, 'HOME': 0.0, 'CONJ': 0.19047619047619047, 'COGPROC': 0.14285714285714285, 'SEXUAL': 0.0, 'AUXVERB': 0.0, 'SHEHE': 0.047619047619047616, 'BIO': 0.0, 'DIFFER': 0.14285714285714285, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.19047619047619047, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.047619047619047616, 'NONFLU': 0.0, 'MALE': 0.047619047619047616, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.38095238095238093, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.047619047619047616, 'I': 0.0, 'IPRON': 0.0, 'SOCIAL': 0.047619047619047616, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.19047619047619047, 'VERB': 0.0, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.047619047619047616, 'YOU': 0.0, 'ADJ': 0.19047619047619047, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.047619047619047616, 'COMPARE': 0.047619047619047616, 'ADVERB': 0.0, 'PRONOUN': 0.047619047619047616, 'MONEY': 0.0, 'FOCUSPRESENT': 0.0, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.14285714285714285, 'CERTAIN': 0.0, 'EMP_RES': -0.007125010476190479, 'UNCIVIL_ABUSE': -0.014427793999999999, 'CONSTRUCTIVENESS': 0.00815491861904762, 'JUSTIFICATION': 0.06214774119047618, 'RECIPROCITY': -0.017789661, 'JUST_EXT': -0.056647770904761904, 'RELEVANCE': 0.07673588157142856, 'JUST_INT': 0.0036339596190476187, 'UNCIV': -0.01442780476190476, 'OFFEN': 0.14099417619047622}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'i know for a fact this ukraine/russia nonsense wouldn’t be going on if all of our smartest citizens weren’t too busy fighting vaccines and could afford to put all their ginormous brains together to help joe biron', 'anger': 0.023255813953488372, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.023255813953488372, 'joy': 0.0, 'negative': 0.023255813953488372, 'positive': 0.046511627906976744, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.023255813953488372, 'PPRON': 0.046511627906976744, 'BODY': 0.0, 'WE': 0.023255813953488372, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.023255813953488372, 'NEGATE': 0.0, 'QUANT': 0.046511627906976744, 'THEY': 0.0, 'AFFECT': 0.023255813953488372, 'RELATIV': 0.06976744186046512, 'HOME': 0.0, 'CONJ': 0.046511627906976744, 'COGPROC': 0.13953488372093023, 'SEXUAL': 0.0, 'AUXVERB': 0.046511627906976744, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.023255813953488372, 'POWER': 0.023255813953488372, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.023255813953488372, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.046511627906976744, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.023255813953488372, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.32558139534883723, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.023255813953488372, 'I': 0.023255813953488372, 'IPRON': 0.023255813953488372, 'SOCIAL': 0.06976744186046512, 'ASSENT': 0.0, 'DRIVES': 0.046511627906976744, 'PERCEPT': 0.0, 'VERB': 0.11627906976744186, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.046511627906976744, 'YOU': 0.0, 'ADJ': 0.023255813953488372, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.023255813953488372, 'COMPARE': 0.023255813953488372, 'ADVERB': 0.023255813953488372, 'PRONOUN': 0.06976744186046512, 'MONEY': 0.0, 'FOCUSPRESENT': 0.06976744186046512, 'INGEST': 0.0, 'AFFILIATION': 0.046511627906976744, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.11627906976744186, 'CERTAIN': 0.06976744186046512, 'EMP_RES': 0.013019212976744186, 'UNCIVIL_ABUSE': -0.02661214572093023, 'CONSTRUCTIVENESS': -0.005030275209302325, 'JUSTIFICATION': 0.0373866041627907, 'RECIPROCITY': 0.0035206399069767453, 'JUST_EXT': -0.03098439297674419, 'RELEVANCE': 0.043417007558139535, 'JUST_INT': 0.006890911302325582, 'UNCIV': -0.026612161953488374, 'OFFEN': 0.24414913255813953}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '@ani india : we urge all the parties involved in ukraine crisis to exercise restrain and maintain peace. we request to establish dialogue and solve the issue diplomatically. india is the land of gandhi. satya ahimsa blah blah..', 'anger': 0.023809523809523808, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.023809523809523808, 'joy': 0.0, 'negative': 0.023809523809523808, 'positive': 0.023809523809523808, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.023809523809523808, 'PPRON': 0.047619047619047616, 'BODY': 0.0, 'WE': 0.047619047619047616, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.023809523809523808, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.047619047619047616, 'HOME': 0.0, 'CONJ': 0.047619047619047616, 'COGPROC': 0.023809523809523808, 'SEXUAL': 0.0, 'AUXVERB': 0.023809523809523808, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.047619047619047616, 'CAUSE': 0.0, 'FILLER': 0.047619047619047616, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.07142857142857142, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.2857142857142857, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.047619047619047616, 'I': 0.0, 'IPRON': 0.0, 'SOCIAL': 0.047619047619047616, 'ASSENT': 0.0, 'DRIVES': 0.047619047619047616, 'PERCEPT': 0.0, 'VERB': 0.023809523809523808, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.0, 'PRONOUN': 0.047619047619047616, 'MONEY': 0.0, 'FOCUSPRESENT': 0.023809523809523808, 'INGEST': 0.0, 'AFFILIATION': 0.047619047619047616, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.09523809523809523, 'CERTAIN': 0.023809523809523808, 'EMP_RES': 0.005846653928571428, 'UNCIVIL_ABUSE': 0.004252014023809526, 'CONSTRUCTIVENESS': 0.008588997047619047, 'JUSTIFICATION': 0.06644317769047621, 'RECIPROCITY': -0.0035654518333333336, 'JUST_EXT': -0.06273498133333334, 'RELEVANCE': 0.10203989595238097, 'JUST_INT': 0.00888237119047619, 'UNCIV': 0.004251997619047619, 'OFFEN': 0.24257289000000007}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'listening to putin’s rant makes absolutely plain that his problem was never with nato or possible membership of 🇺🇦. it was always about the collapse of the ussr and the independence of ukraine.', 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.02631578947368421, 'joy': 0.0, 'negative': 0.02631578947368421, 'positive': 0.0, 'sadness': 0.02631578947368421, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.02631578947368421, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.02631578947368421, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.02631578947368421, 'HOME': 0.0, 'CONJ': 0.05263157894736842, 'COGPROC': 0.07894736842105263, 'SEXUAL': 0.0, 'AUXVERB': 0.05263157894736842, 'SHEHE': 0.02631578947368421, 'BIO': 0.0, 'DIFFER': 0.02631578947368421, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.02631578947368421, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.05263157894736842, 'ANGER': 0.0, 'ARTICLE': 0.07894736842105263, 'NONFLU': 0.0, 'MALE': 0.02631578947368421, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.4473684210526316, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.0, 'I': 0.0, 'IPRON': 0.05263157894736842, 'SOCIAL': 0.05263157894736842, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.02631578947368421, 'VERB': 0.10526315789473684, 'HEAR': 0.02631578947368421, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.02631578947368421, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.02631578947368421, 'COMPARE': 0.0, 'ADVERB': 0.05263157894736842, 'PRONOUN': 0.07894736842105263, 'MONEY': 0.0, 'FOCUSPRESENT': 0.05263157894736842, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.02631578947368421, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.15789473684210525, 'CERTAIN': 0.02631578947368421, 'EMP_RES': 0.0065441614473684215, 'UNCIVIL_ABUSE': -0.020257936894736844, 'CONSTRUCTIVENESS': 0.007419130236842104, 'JUSTIFICATION': 0.06600648642105264, 'RECIPROCITY': -0.0018244657631578938, 'JUST_EXT': -0.06080462268421053, 'RELEVANCE': 0.08806891889473685, 'JUST_INT': 0.013298905500000001, 'UNCIV': -0.020257923684210522, 'OFFEN': 0.24995218684210527}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"ukraine is not a real country tho come on. the same way that pakistan is not a real country. it's just a western colony to keep a check on an enemy next door.\", 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0, 'positive': 0.05555555555555555, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.05555555555555555, 'PPRON': 0.0, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.05555555555555555, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.1388888888888889, 'HOME': 0.0, 'CONJ': 0.027777777777777776, 'COGPROC': 0.05555555555555555, 'SEXUAL': 0.0, 'AUXVERB': 0.05555555555555555, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.05555555555555555, 'POWER': 0.0, 'NETSPEAK': 0.027777777777777776, 'INFORMAL': 0.027777777777777776, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.027777777777777776, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.16666666666666666, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.4722222222222222, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.08333333333333333, 'I': 0.0, 'IPRON': 0.05555555555555555, 'SOCIAL': 0.0, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.0, 'VERB': 0.1111111111111111, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.05555555555555555, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.027777777777777776, 'ADVERB': 0.05555555555555555, 'PRONOUN': 0.05555555555555555, 'MONEY': 0.0, 'FOCUSPRESENT': 0.1111111111111111, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.027777777777777776, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.08333333333333333, 'CERTAIN': 0.0, 'EMP_RES': -0.010383660500000001, 'UNCIVIL_ABUSE': -0.01374292841666667, 'CONSTRUCTIVENESS': 0.005619815944444443, 'JUSTIFICATION': 0.08140653225, 'RECIPROCITY': -0.02688706758333333, 'JUST_EXT': -0.071177054, 'RELEVANCE': 0.08586084666666666, 'JUST_INT': 0.006035265361111111, 'UNCIV': -0.01374293888888889, 'OFFEN': 0.29337691055555554}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"russia literally invading ukraine and the media just can't absolutely fucking can't say it. \\n\\nmore than anything the goddamn press has failed us. \", 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0, 'positive': 0.0, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.04, 'BODY': 0.0, 'WE': 0.04, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.04, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.08, 'QUANT': 0.04, 'THEY': 0.0, 'AFFECT': 0.04, 'RELATIV': 0.0, 'HOME': 0.0, 'CONJ': 0.04, 'COGPROC': 0.12, 'SEXUAL': 0.04, 'AUXVERB': 0.04, 'SHEHE': 0.0, 'BIO': 0.04, 'DIFFER': 0.12, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.04, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.04, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.04, 'ARTICLE': 0.08, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.4, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.0, 'I': 0.0, 'IPRON': 0.04, 'SOCIAL': 0.08, 'ASSENT': 0.0, 'DRIVES': 0.04, 'PERCEPT': 0.08, 'VERB': 0.12, 'HEAR': 0.04, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.08, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.08, 'ADVERB': 0.04, 'PRONOUN': 0.08, 'MONEY': 0.0, 'FOCUSPRESENT': 0.2, 'INGEST': 0.0, 'AFFILIATION': 0.04, 'SWEAR': 0.04, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.04, 'CERTAIN': 0.0, 'EMP_RES': -0.022296368159999996, 'UNCIVIL_ABUSE': 0.02479878628, 'CONSTRUCTIVENESS': -0.0014318864000000015, 'JUSTIFICATION': 0.054122026319999995, 'RECIPROCITY': -0.013783497760000001, 'JUST_EXT': -0.05301235516, 'RELEVANCE': 0.08564589908, 'JUST_INT': 0.02107772628, 'UNCIV': 0.024798792, 'OFFEN': 0.52941556}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"@nikkihaley let's just compromise by letting russian have the people in our government that were doing shady deals with ukraine\", 'anger': 0.0, 'anticipation': 0.05, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.05, 'negative': 0.0, 'positive': 0.1, 'sadness': 0.0, 'surprise': 0.05, 'trust': 0.1, 'PPRON': 0.05, 'BODY': 0.0, 'WE': 0.05, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.05, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.25, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.05, 'ANGER': 0.0, 'ARTICLE': 0.05, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.6, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.05, 'I': 0.0, 'IPRON': 0.05, 'SOCIAL': 0.05, 'ASSENT': 0.0, 'DRIVES': 0.05, 'PERCEPT': 0.0, 'VERB': 0.25, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.05, 'PRONOUN': 0.1, 'MONEY': 0.0, 'FOCUSPRESENT': 0.1, 'INGEST': 0.0, 'AFFILIATION': 0.05, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.15, 'CERTAIN': 0.0, 'EMP_RES': 0.0128944003, 'UNCIVIL_ABUSE': -0.012639203900000002, 'CONSTRUCTIVENESS': 0.0052631055, 'JUSTIFICATION': 0.0516697529, 'RECIPROCITY': 0.00481986135, 'JUST_EXT': -0.0309770173, 'RELEVANCE': 0.0594027082, 'JUST_INT': 0.007290079150000001, 'UNCIV': -0.012639210000000001, 'OFFEN': 0.3582146500000001}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '“kenya &amp; almost every african country was birthed by the ending of empire. our borders were not of our own drawing…at independence had we chosen to pursue states on the basis of ethnic racial or religious homogeneity we would still be waging bloody wars many decades later” ', 'anger': 0.0, 'anticipation': 0.018518518518518517, 'disgust': 0.0, 'fear': 0.037037037037037035, 'joy': 0.018518518518518517, 'negative': 0.018518518518518517, 'positive': 0.037037037037037035, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.018518518518518517, 'PPRON': 0.07407407407407407, 'BODY': 0.0, 'WE': 0.07407407407407407, 'DEATH': 0.018518518518518517, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.018518518518518517, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.018518518518518517, 'RELATIV': 0.09259259259259259, 'HOME': 0.0, 'CONJ': 0.018518518518518517, 'COGPROC': 0.07407407407407407, 'SEXUAL': 0.0, 'AUXVERB': 0.09259259259259259, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.037037037037037035, 'POWER': 0.018518518518518517, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.018518518518518517, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.05555555555555555, 'ANGER': 0.018518518518518517, 'ARTICLE': 0.037037037037037035, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.018518518518518517, 'FRIEND': 0.0, 'FUNCTION': 0.4074074074074074, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.037037037037037035, 'I': 0.0, 'IPRON': 0.0, 'SOCIAL': 0.07407407407407407, 'ASSENT': 0.0, 'DRIVES': 0.09259259259259259, 'PERCEPT': 0.0, 'VERB': 0.09259259259259259, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.018518518518518517, 'YOU': 0.0, 'ADJ': 0.018518518518518517, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.037037037037037035, 'COMPARE': 0.018518518518518517, 'ADVERB': 0.037037037037037035, 'PRONOUN': 0.07407407407407407, 'MONEY': 0.018518518518518517, 'FOCUSPRESENT': 0.018518518518518517, 'INGEST': 0.0, 'AFFILIATION': 0.07407407407407407, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.05555555555555555, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.12962962962962962, 'CERTAIN': 0.0, 'EMP_RES': 0.0023979432037037045, 'UNCIVIL_ABUSE': -0.018922173777777777, 'CONSTRUCTIVENESS': -0.0018217448518518518, 'JUSTIFICATION': 0.04426060477777778, 'RECIPROCITY': -0.0017196781296296292, 'JUST_EXT': -0.034673229166666666, 'RELEVANCE': 0.053186255555555555, 'JUST_INT': 0.005767165074074073, 'UNCIV': -0.018922187592592593, 'OFFEN': 0.2936610092592593}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '@reuters best expedite that nato status.', 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0, 'positive': 0.14285714285714285, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.0, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.14285714285714285, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.14285714285714285, 'RELATIV': 0.0, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.0, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.2857142857142857, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.0, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.14285714285714285, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.0, 'I': 0.0, 'IPRON': 0.14285714285714285, 'SOCIAL': 0.0, 'ASSENT': 0.0, 'DRIVES': 0.2857142857142857, 'PERCEPT': 0.0, 'VERB': 0.0, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.14285714285714285, 'ACHIEVE': 0.14285714285714285, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.14285714285714285, 'ADVERB': 0.0, 'PRONOUN': 0.14285714285714285, 'MONEY': 0.0, 'FOCUSPRESENT': 0.0, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.14285714285714285, 'ANX': 0.0, 'PREP': 0.0, 'CERTAIN': 0.0, 'EMP_RES': 0.02679211542857143, 'UNCIVIL_ABUSE': -0.019360950999999998, 'CONSTRUCTIVENESS': -0.007489690857142858, 'JUSTIFICATION': 0.05747464514285715, 'RECIPROCITY': -0.02295637485714286, 'JUST_EXT': -0.05004330842857143, 'RELEVANCE': 0.07103926114285715, 'JUST_INT': 0.013608289285714285, 'UNCIV': -0.01936094571428571, 'OFFEN': 0.2404715857142857}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"@pnwpragmatist @tulsigabbard she's putin's useful idiot. hillary was absolutely right about her.\", 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.07142857142857142, 'fear': 0.0, 'joy': 0.0, 'negative': 0.07142857142857142, 'positive': 0.0, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0, 'PPRON': 0.14285714285714285, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.0, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.07142857142857142, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.07142857142857142, 'SEXUAL': 0.0, 'AUXVERB': 0.07142857142857142, 'SHEHE': 0.14285714285714285, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.07142857142857142, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.07142857142857142, 'ANGER': 0.0, 'ARTICLE': 0.0, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.2857142857142857, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.07142857142857142, 'I': 0.0, 'IPRON': 0.0, 'SOCIAL': 0.14285714285714285, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.0, 'VERB': 0.14285714285714285, 'HEAR': 0.0, 'FEMALE': 0.14285714285714285, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.07142857142857142, 'PRONOUN': 0.14285714285714285, 'MONEY': 0.0, 'FOCUSPRESENT': 0.07142857142857142, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.07142857142857142, 'CERTAIN': 0.0, 'EMP_RES': 0.0071003515, 'UNCIVIL_ABUSE': -0.02173539207142857, 'CONSTRUCTIVENESS': 0.004567715000000001, 'JUSTIFICATION': 0.06220060092857143, 'RECIPROCITY': -0.005820966214285714, 'JUST_EXT': -0.04542868707142857, 'RELEVANCE': 0.06754238942857142, 'JUST_INT': 0.023059911428571427, 'UNCIV': -0.021735392857142856, 'OFFEN': 0.32386613428571426}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"@lauraforczyk what happens in space stays in space. i don't see no conflict with the iss and the russia ukraine conflict interrupt there missions\", 'anger': 0.12, 'anticipation': 0.04, 'disgust': 0.0, 'fear': 0.08, 'joy': 0.0, 'negative': 0.12, 'positive': 0.0, 'sadness': 0.08, 'surprise': 0.04, 'trust': 0.0, 'PPRON': 0.04, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.04, 'NUMBER': 0.0, 'POSEMO': 0.0, 'NEGATE': 0.08, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0, 'RELATIV': 0.2, 'HOME': 0.0, 'CONJ': 0.04, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.04, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.04, 'SEE': 0.04, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.08, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.44, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.2, 'I': 0.04, 'IPRON': 0.04, 'SOCIAL': 0.0, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.04, 'VERB': 0.16, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.04, 'PRONOUN': 0.08, 'MONEY': 0.0, 'FOCUSPRESENT': 0.16, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.12, 'CERTAIN': 0.0, 'EMP_RES': 0.01634699448, 'UNCIVIL_ABUSE': -0.020766094520000004, 'CONSTRUCTIVENESS': 0.0229637276, 'JUSTIFICATION': 0.0624713698, 'RECIPROCITY': 0.003977135840000001, 'JUST_EXT': -0.05240285872, 'RELEVANCE': 0.08349827484, 'JUST_INT': 0.01684857632, 'UNCIV': -0.020766096799999998, 'OFFEN': 0.2693579544}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3e7099eac5d5>:33: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n",
            "<ipython-input-10-3e7099eac5d5>:77: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(lexicon)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': '@lazypajamas @countrybol @alvarosilva1906 i don’t support nato i am no fan of nato.', 'anger': 0.0, 'anticipation': 0.0, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.0, 'negative': 0.0, 'positive': 0.0625, 'sadness': 0.0, 'surprise': 0.0, 'trust': 0.0625, 'PPRON': 0.125, 'BODY': 0.0, 'WE': 0.0, 'DEATH': 0.0, 'FOCUSFUTURE': 0.0, 'FEEL': 0.0, 'INTERROG': 0.0, 'NUMBER': 0.0, 'POSEMO': 0.0625, 'NEGATE': 0.0625, 'QUANT': 0.0, 'THEY': 0.0, 'AFFECT': 0.0625, 'RELATIV': 0.0, 'HOME': 0.0, 'CONJ': 0.0, 'COGPROC': 0.0, 'SEXUAL': 0.0, 'AUXVERB': 0.0625, 'SHEHE': 0.0, 'BIO': 0.0, 'DIFFER': 0.0, 'POWER': 0.0, 'NETSPEAK': 0.0, 'INFORMAL': 0.0, 'CAUSE': 0.0, 'FILLER': 0.0, 'INSIGHT': 0.0, 'LEISURE': 0.0, 'NEGEMO': 0.0, 'MOTION': 0.0, 'SEE': 0.0, 'FOCUSPAST': 0.0, 'ANGER': 0.0, 'ARTICLE': 0.0, 'NONFLU': 0.0, 'MALE': 0.0, 'WORK': 0.0, 'FRIEND': 0.0, 'FUNCTION': 0.3125, 'RISK': 0.0, 'FAMILY': 0.0, 'SPACE': 0.0, 'I': 0.125, 'IPRON': 0.0, 'SOCIAL': 0.0, 'ASSENT': 0.0, 'DRIVES': 0.0, 'PERCEPT': 0.0, 'VERB': 0.125, 'HEAR': 0.0, 'FEMALE': 0.0, 'DISCREP': 0.0, 'YOU': 0.0, 'ADJ': 0.0, 'ACHIEVE': 0.0, 'RELIG': 0.0, 'TENTAT': 0.0, 'COMPARE': 0.0, 'ADVERB': 0.0, 'PRONOUN': 0.125, 'MONEY': 0.0, 'FOCUSPRESENT': 0.125, 'INGEST': 0.0, 'AFFILIATION': 0.0, 'SWEAR': 0.0, 'HEALTH': 0.0, 'SAD': 0.0, 'TIME': 0.0, 'REWARD': 0.0, 'ANX': 0.0, 'PREP': 0.0625, 'CERTAIN': 0.0, 'EMP_RES': 0.0291536065625, 'UNCIVIL_ABUSE': -0.029473828, 'CONSTRUCTIVENESS': -0.0152018385625, 'JUSTIFICATION': 0.0502005168125, 'RECIPROCITY': -0.0052129674374999995, 'JUST_EXT': -0.0413381680625, 'RELEVANCE': 0.046457488562499996, 'JUST_INT': 0.0075658454375000005, 'UNCIV': -0.029473850000000003, 'OFFEN': 0.334731}\n",
            "Writing emolized messages to csv...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-937f7aca29cc>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfeature_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1496\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_newdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfeature_start\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_newdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_scalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_newdata' is not defined"
          ]
        }
      ],
      "source": [
        "filename = \"/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/data/test_data2.csv\"\n",
        "DATA_PATH = \"/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/data/test_data2.csv\"\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/data'     # You'll get 2 directories here, one will have t\n",
        "X_col = 'text'  # Name of X column (string)\n",
        "y_col = 'label'        # Name of y column (0/1)\n",
        "MODIFIED_DATA = os.path.join(OUTPUT_DIR, '/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/data')\n",
        "os.makedirs(MODIFIED_DATA, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "extract_feats(filename,\"text\")\n",
        "\n",
        "model = joblib.load('/content/drive/MyDrive/Colab_Notebooks/deliberative-politics-main/lexica/final_model.pkl') # Downloaded from https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/classifier/final_model.pkl\n",
        "\n",
        "feature_start, num_ftrs = 16,1496\n",
        "X = df_newdata.iloc[:, feature_start:feature_start+num_ftrs]\n",
        "print(df_newdata.shape)\n",
        "X_scaled = all_scalers[feature].transform(X)\n",
        "y = predictions(X_scaled,model)\n",
        "unlabeled_df['predicted_' + feature] = y\n",
        "print(\"Printing predicted values:\")\n",
        "with open('generated_labels.csv', 'a',newline='') as csvfile:\n",
        "    f = csv.writer(csvfile, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    for i,t in enumerate(tweets):\n",
        "         f.writerow([class_to_name(y[i])])\n",
        "#        for i,t in enumerate(tweets):\n",
        "#            print(t)\n",
        "#            print(class_to_name(y[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJO6HwGaxkSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgGRk2VTyt90"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVmURycRyFnL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6XSnEUM882Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBHDiB-E-aK_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KsFJ8Qfx-Lgd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}